{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Code for Author Profiling (Machine Learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this notebook it will be explained how a dataset of Twitter post available for the PAN2015 Author Profiling (AP) competition was encoded in a Word2Vec structure. These word embeddings were later used to train several Machine Learning (ML) algorithms to test their accuracy predicting the targets of the aformentioned dataset (Age, Gender and Personality traits). Several Python example codes available in GitHub, as well as ML libraries like scikit-learn and Keras were used to develop the current project.\n",
    "\n",
    "The contents of the notebook is as follows:\n",
    "1. Preprocesing of the dataset available in xml format (tokenize and fill arrays).\n",
    "2. Create a Word2Vec model from the vocabulary of the dataset\n",
    "3. Train Extra trees, Random Forest, Support Vector Machines, Gaussian Naive Bayes and a Recurrent Neural Network (Long short-term memory) with the Word Embeddings from Word2Vec\n",
    "4. Test and record Accuracy and Root Mean Squared Error (RMSE) from the performance of the ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets import the classes and libraries needed for the project, then we can create a Word2Vec model from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "class User:\n",
    "\tdef __init__(self, user_id):\n",
    "\t\tself.user_id = user_id\n",
    "\t\tself.documents = []\n",
    "\n",
    "\tdef add_document(self, text):\n",
    "\t\tself.documents.append(text)\n",
    "\n",
    "\tdef get_documents(self):\n",
    "\t\treturn self.documents\n",
    "\n",
    "\n",
    "def load_users(path):\n",
    "    users = []\n",
    "    files = os.listdir(path)\n",
    "    files.remove('truth.txt')\n",
    "    for xml_filename in files:\n",
    "        tree = ET.parse(path + xml_filename)\n",
    "        root = tree.getroot()\n",
    "        user_id = root.attrib['id']\n",
    "        user = User(user_id)\n",
    "\n",
    "        # add tweets to user\n",
    "        for child in root:\n",
    "            user.add_document(child.text)\n",
    "\n",
    "        users.append(user)\n",
    "\n",
    "    return users\n",
    "\n",
    "def get_users_truth(path):\n",
    "    users_truth = {}\n",
    "    f = open(path + 'truth.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        toks = line.split(':::')\n",
    "        users_truth[toks[0]] = [toks[1], toks[2], toks[3], toks[4], toks[5], toks[6], toks[7]]\n",
    "\n",
    "    return users_truth\n",
    "\n",
    "\n",
    "#path must have the directory where the dataset is saved\n",
    "#users and truth have the twitter posts and the labels of the samples\n",
    "#the whole dataset is tokenized to create the Word2Vec embeddings\n",
    "\n",
    "path = '/working directory/pan15-author-profiling-training-dataset-english-2015-03-02/'\n",
    "users = load_users(path)\n",
    "truth = get_users_truth(path)\n",
    "\n",
    "def get_all_documents(users):\n",
    "    docs = {}\n",
    "\n",
    "    for user in users:\n",
    "        rez = []\n",
    "        for x in user.get_documents():\n",
    "            rez = rez + re.findall(r\"\\w+\", x)\n",
    "\n",
    "        docs[user] = rez\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "docs = {}\n",
    "\n",
    "\n",
    "def get_tokens_per_gender(users):\n",
    "    user_sentences = get_all_documents(users)\n",
    "\n",
    "    texts_M = []\n",
    "    texts_F = []\n",
    "    all_texts = []\n",
    "\n",
    "    for user, doc in iter(user_sentences.items()):\n",
    "        rez = [x.lower() for x in doc ]\n",
    "        rez_stop = [x.lower() for x in doc]\n",
    "        all_texts.append(rez_stop)\n",
    "        if (truth[user.user_id][0] == 'M'):\n",
    "            texts_M += rez\n",
    "        else:\n",
    "            texts_F += rez\n",
    "    return [texts_M, texts_F, all_texts]\n",
    "\n",
    "\n",
    "def get_tokens(users):\n",
    "    user_sentences = get_all_documents(users)\n",
    "    all_texts = []\n",
    "\n",
    "    for user, doc in iter(user_sentences.items()):\n",
    "        rez_stop = [x.lower() for x in doc]\n",
    "        all_texts.append(rez_stop)\n",
    "\n",
    "    return [all_texts]\n",
    "\n",
    "def get_trgts(users, truth, trgt):\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for user in users:\n",
    "        if trgt > 1:\n",
    "            labels.append(float(truth[user.user_id][trgt]))\n",
    "        else:\n",
    "            labels.append(truth[user.user_id][trgt])\n",
    "\n",
    "    return [labels]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the Word2Vec model is created and functions to vectorize the documents from the samples are defined.\n",
    "\"y_raw = get_trgts(users, truth, 1)\" 1 means the 2nd label, labels are drawn from \"truth\" array from 0 to 6. \n",
    "0-Gender, 1-Age range, (2-6 Personality traits, float values) 2-(E), 3-(S), 4-(A), 5-(C), 6-(O)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddings(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(list(word2vec.values())[0])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "\n",
    "class TfidfEmbeddings(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                     for w in words if w in self.word2vec] or\n",
    "                    [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "m, f, all_text = get_tokens_per_gender(users)\n",
    "y_raw = get_trgts(users, truth, 1) #get each target one at a time to create different classifiers\n",
    "y = y_raw[0]\n",
    "X = all_text\n",
    "\n",
    "\n",
    "# size : embed dimension\n",
    "# min_count : filter words without min frequency\n",
    "# sg : 0 for CBOW; 1 for skip-gram\n",
    "model = Word2Vec(all_text, size=500, min_count=30, sg=1, workers=cpu_count(), window=10) #paraneters found through a grid search\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the classifiers are trained using a pipeline to fit the algorithms with the embeddings from Word2Vec.\n",
    "There are 2 type of algorithms, Classifiers (Cs) and Regressors (Rs), Cs are used for labes 0,1 and Rs for labels (2-6). for the Cs a One-Vs-the_Rest strategy is used to tackle multiclass problems. A grid search is performed in the SVM classifier. \n",
    "Note. This code must be executed once for each label.\n",
    "\n",
    "Lets see the Cs first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OVR_RF = OneVsRestClassifier(RandomForestClassifier(n_estimators=200))\n",
    "CRnd_Frst_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddings(w2v)),\n",
    "                        (\"Random Forest Cl tfidf\", OVR_RF)])\n",
    "\n",
    "OVR_ET = OneVsRestClassifier(ExtraTreesClassifier(n_estimators=200))\n",
    "CExt_Trees_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddings(w2v)),\n",
    "                        (\"Extra Trees Cl tfidf\", OVR_ET)])\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':[0.5, 0.9, 1, 10], 'degree':[1, 3]}\n",
    "svm = SVC()\n",
    "clf = GridSearchCV(svm, parameters)\n",
    "clf = OneVsRestClassifier(clf)\n",
    "CSVM_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddings(w2v)),\n",
    "                        (\"SVM tfidf\", clf)])\n",
    "\n",
    "OVR_GNB = OneVsRestClassifier(GaussianNB())\n",
    "CGNB_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddings(w2v)),\n",
    "                        (\"SVM tfidf\", OVR_GNB)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the Rs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RRnd_Frst_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddings(w2v)),\n",
    "                        (\"Random Forest Rg tfidf\", RandomForestRegressor(n_estimators=100))])\n",
    "\n",
    "RExt_Trees_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddings(w2v)),\n",
    "                        (\"Extra Trees Rg tfidf\", ExtraTreesRegressor(n_estimators=100, max_features='auto'))])\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':[0.5, 0.9, 1, 10], 'degree':[1, 3], 'epsilon':[0.1, 0.2, 0.3]}\n",
    "svr = SVR()\n",
    "svr = GridSearchCV(svr, parameters)\n",
    "RSVR_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddings(w2v)),\n",
    "                        (\"Support Vector Rg tfidf\", svr)])\n",
    "\n",
    "RBay_Rdg_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddings(w2v)),\n",
    "                        (\"Bayesian Ridge Rg tfidf\", linear_model.BayesianRidge())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the algorithms can execute on the test partition of the dataset. A 10-fold cross valdation was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classifiers\n",
    "\n",
    "scoring = 'accuracy'\n",
    "folds = 10\n",
    "\n",
    "print(cross_val_score(CRnd_Frst_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())\n",
    "print(cross_val_score(CExt_Trees_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())\n",
    "print(cross_val_score(CSVM_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())\n",
    "print(cross_val_score(CGNB_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regressors\n",
    "\n",
    "scoring = 'neg_mean_squared_error'\n",
    "folds = 10\n",
    "\n",
    "print(np.math.sqrt(abs(cross_val_score(RRnd_Frst_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())))\n",
    "print(np.math.sqrt(abs(cross_val_score(RExt_Trees_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())))\n",
    "print(np.math.sqrt(abs(cross_val_score(RSVR_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())))\n",
    "print(np.math.sqrt(abs(cross_val_score(RBay_Rdg_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Recurrent Neural Network (RNN-LSTM) a few modifications to the code above were made. First the Word2Vec model was not averaged to represent each document from the samples. The RNN approach used each document converted word by word to their Word2Vec numerical vector and then partitioned into Train and Test slices, using the same 10-fold cross validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from numpy.ma import zeros\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequential models in Keras only accept numeric values, so the labels must be converted for the One-Vs-the-Rest strategy on the classification sub-problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trgts_num(users, truth, trgt):\n",
    "    labels = []\n",
    "    for user in users:\n",
    "        if trgt > 1:\n",
    "            labels.append(float(truth[user.user_id][trgt]))\n",
    "        else:\n",
    "            if trgt == 0:\n",
    "                if truth[user.user_id][trgt] == 'M':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "            else:\n",
    "                if truth[user.user_id][trgt] == '18-24':\n",
    "                    labels.append(0)\n",
    "                elif truth[user.user_id][trgt] == '25-34':\n",
    "                    labels.append(0)\n",
    "                elif truth[user.user_id][trgt] == '35-49':\n",
    "                    labels.append(0)\n",
    "                elif truth[user.user_id][trgt] == '50-XX':\n",
    "                    labels.append(1)\n",
    "    return [labels]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM models in Keras only accept fixed-length inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2Vec number of features\n",
    "num_features = 500\n",
    "# Limit each newsline to a fixed number of words\n",
    "document_max_num_words = 100\n",
    "num_categories = 1 # 2, 3, etc.\n",
    "\n",
    "document_X = {}\n",
    "document_Y = {}\n",
    "\n",
    "w2v_model = Word2Vec(all_text, size=num_features, min_count=5, sg=1, window=10, workers=cpu_count())\n",
    "w2v_model.init_sims(replace=True)\n",
    "num_samples = len(all_text)\n",
    "\n",
    "X = zeros(shape=(num_samples, document_max_num_words, num_features)).astype('float32')\n",
    "Y = zeros(shape=(num_samples, num_categories)).astype('float32')\n",
    "\n",
    "empty_word = zeros(num_features).astype('float32')\n",
    "\n",
    "\n",
    "for idx, document in enumerate(all_text):\n",
    "    for jdx, word in enumerate(document):\n",
    "        if jdx == document_max_num_words:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            if word in w2v_model:\n",
    "                X[idx, jdx, :] = w2v_model[word]\n",
    "            else:\n",
    "                X[idx, jdx, :] = empty_word\n",
    "\n",
    "\n",
    "for idx, key in enumerate(labels):\n",
    "    Y[idx, :] = key\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two type of LSTM models need to be defined in order to solve Classification and Regression problems.\n",
    "The Classification sub-problem is engaged by this model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10) \n",
    "cvscores = []\n",
    "for train, test in kf.split(X, Y):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(int(document_max_num_words*1.5), input_shape=(document_max_num_words, num_features)))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(num_categories))\n",
    "\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X[train], Y[train], epochs=10, batch_size=16, verbose=0)\n",
    "\n",
    "    score, acc = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    cvscores.append(acc)\n",
    "\n",
    "print(np.mean(cvscores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the LSTM model for regression is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(int(document_max_num_words * 1.5), input_shape=(document_max_num_words, num_features)))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adamax\") \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=10, batch_size=16, verbose=0)\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "rmse = 0.0\n",
    "for x in results:\n",
    "    print(np.math.sqrt(abs(x)))\n",
    "    rmse += np.math.sqrt(abs(x))\n",
    "rmse /= len(results)\n",
    "print(\"Results: %.4f (%.4f) MSE\" % (results.mean(), results.std()))\n",
    "print(\"Results: %.4f RMSE\" % (rmse))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
